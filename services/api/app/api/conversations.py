"""Conversation management API endpoints - ç»Ÿä¸€æ„å›¾è·¯ç”±æ¶æ„"""
import logging
import uuid
from typing import Optional, List
from fastapi import APIRouter, Depends, HTTPException, status
from pydantic import BaseModel
from sqlalchemy.orm import Session
from sqlalchemy.orm.attributes import flag_modified
from openai import OpenAI

from ..db import get_db
from ..services.conversation_service import ConversationService
from ..services.retrieval import search
from ..services.intent_recognizer import (
    recognize_intent, get_intent_recognizer, IntentType, ActionType
)
from ..services.dynamic_interaction import (
    get_dynamic_interaction_service, DynamicQuestion, QuestionType
)
from ..models.user import User
from ..models.config import InteractionFlow, InteractionSession
from ..config import settings
from .auth import get_current_user, get_current_user_optional

logger = logging.getLogger(__name__)

# Initialize OpenAI client
_llm_client = None

def get_llm_client():
    global _llm_client
    if _llm_client is None:
        _llm_client = OpenAI(
            api_key=settings.upstream_llm_key,
            base_url=settings.upstream_llm_url.replace("/chat/completions", ""),
        )
    return _llm_client

router = APIRouter(prefix="/api/conversations", tags=["conversations"])


# ==================== Request/Response Models ====================

class CreateConversationRequest(BaseModel):
    title: Optional[str] = None
    scenario_id: Optional[str] = None


class UpdateConversationRequest(BaseModel):
    title: Optional[str] = None
    tags: Optional[List[str]] = None
    scenario_id: Optional[str] = None


class SendMessageRequest(BaseModel):
    content: str


class FeedbackRequest(BaseModel):
    feedback: str  # positive/negative
    feedback_text: Optional[str] = None


class CreateShareRequest(BaseModel):
    allow_copy: bool = True
    expires_in_days: Optional[int] = None


class ConversationResponse(BaseModel):
    id: int
    conversation_id: str
    title: Optional[str]
    summary: Optional[str]
    status: str
    is_pinned: bool
    message_count: int
    last_message_at: Optional[str]
    tags: List
    scenario_id: Optional[str]
    created_at: Optional[str]
    updated_at: Optional[str]


class InteractionTriggerInfo(BaseModel):
    """äº¤äº’æµç¨‹è§¦å‘ä¿¡æ¯ï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰"""
    flow_id: str
    flow_name: str
    description: Optional[str] = None
    confidence: float
    reason: str


class DynamicInteractionQuestion(BaseModel):
    """åŠ¨æ€ç”Ÿæˆçš„é—®é¢˜"""
    field_id: str
    question: str
    question_type: str  # single/multi/input/confirm
    options: Optional[List[dict]] = None
    placeholder: Optional[str] = None


class DynamicInteractionInfo(BaseModel):
    """åŠ¨æ€äº¤äº’ä¿¡æ¯"""
    session_id: str
    is_dynamic: bool = True
    question: Optional[DynamicInteractionQuestion] = None
    progress: Optional[dict] = None


class MessageResponse(BaseModel):
    model_config = {"protected_namespaces": ()}
    
    id: int
    message_id: str
    role: str
    content: str
    sources: List
    feedback: Optional[str]
    tokens_used: Optional[int]
    model_used: Optional[str]
    latency_ms: Optional[int]
    created_at: Optional[str]
    # äº¤äº’æµç¨‹è§¦å‘ä¿¡æ¯ï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰
    interaction_trigger: Optional[InteractionTriggerInfo] = None
    # åŠ¨æ€äº¤äº’ä¿¡æ¯ï¼ˆæ–°ç‰ˆï¼‰
    interaction: Optional[DynamicInteractionInfo] = None


class ConversationListResponse(BaseModel):
    pinned: List[ConversationResponse]
    today: List[ConversationResponse]
    yesterday: List[ConversationResponse]
    this_week: List[ConversationResponse]
    earlier: List[ConversationResponse]


# ==================== API Endpoints ====================

@router.get("", response_model=ConversationListResponse)
async def list_conversations(
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """è·å–ç”¨æˆ·çš„å¯¹è¯åˆ—è¡¨ï¼ˆåˆ†ç»„ï¼‰"""
    service = ConversationService(db)
    groups = service.list_conversations_grouped(str(user.id))
    
    return ConversationListResponse(
        pinned=[ConversationResponse(**c.to_dict()) for c in groups["pinned"]],
        today=[ConversationResponse(**c.to_dict()) for c in groups["today"]],
        yesterday=[ConversationResponse(**c.to_dict()) for c in groups["yesterday"]],
        this_week=[ConversationResponse(**c.to_dict()) for c in groups["this_week"]],
        earlier=[ConversationResponse(**c.to_dict()) for c in groups["earlier"]]
    )


@router.post("", response_model=ConversationResponse)
async def create_conversation(
    body: CreateConversationRequest,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """åˆ›å»ºæ–°å¯¹è¯"""
    service = ConversationService(db)
    conv = service.create_conversation(
        user_id=str(user.id),
        title=body.title,
        scenario_id=body.scenario_id
    )
    return ConversationResponse(**conv.to_dict())


@router.get("/search")
async def search_conversations(
    q: str,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """æœç´¢å†å²å¯¹è¯"""
    service = ConversationService(db)
    results = service.search_conversations(str(user.id), q)
    return {"results": [c.to_dict() for c in results]}


@router.get("/{conversation_id}", response_model=ConversationResponse)
async def get_conversation(
    conversation_id: str,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """è·å–å¯¹è¯è¯¦æƒ…"""
    service = ConversationService(db)
    conv = service.get_conversation(conversation_id, str(user.id))
    
    if not conv:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Conversation not found"
        )
    
    return ConversationResponse(**conv.to_dict())


@router.put("/{conversation_id}", response_model=ConversationResponse)
async def update_conversation(
    conversation_id: str,
    body: UpdateConversationRequest,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """æ›´æ–°å¯¹è¯"""
    service = ConversationService(db)
    conv = service.update_conversation(
        conversation_id,
        str(user.id),
        title=body.title,
        tags=body.tags,
        scenario_id=body.scenario_id
    )
    
    if not conv:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Conversation not found"
        )
    
    return ConversationResponse(**conv.to_dict())


@router.delete("/{conversation_id}")
async def delete_conversation(
    conversation_id: str,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """åˆ é™¤å¯¹è¯"""
    service = ConversationService(db)
    success = service.delete_conversation(conversation_id, str(user.id))
    
    if not success:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Conversation not found"
        )
    
    return {"message": "Conversation deleted"}


@router.post("/{conversation_id}/archive", response_model=ConversationResponse)
async def archive_conversation(
    conversation_id: str,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """å½’æ¡£å¯¹è¯"""
    service = ConversationService(db)
    conv = service.archive_conversation(conversation_id, str(user.id))
    
    if not conv:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Conversation not found"
        )
    
    return ConversationResponse(**conv.to_dict())


@router.post("/{conversation_id}/pin", response_model=ConversationResponse)
async def pin_conversation(
    conversation_id: str,
    pinned: bool = True,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """ç½®é¡¶/å–æ¶ˆç½®é¡¶"""
    service = ConversationService(db)
    conv = service.pin_conversation(conversation_id, str(user.id), pinned)
    
    if not conv:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Conversation not found"
        )
    
    return ConversationResponse(**conv.to_dict())


# ==================== Message Endpoints ====================

@router.get("/{conversation_id}/messages")
async def get_messages(
    conversation_id: str,
    limit: int = 50,
    offset: int = 0,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """è·å–å¯¹è¯æ¶ˆæ¯åˆ—è¡¨"""
    service = ConversationService(db)
    
    # éªŒè¯ç”¨æˆ·æœ‰æƒé™è®¿é—®è¯¥å¯¹è¯
    conv = service.get_conversation(conversation_id, str(user.id))
    if not conv:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Conversation not found"
        )
    
    messages = service.get_messages(conversation_id, limit, offset)
    return {"messages": [m.to_dict() for m in messages]}


@router.post("/{conversation_id}/messages", response_model=MessageResponse)
async def send_message(
    conversation_id: str,
    body: SendMessageRequest,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    å‘é€æ¶ˆæ¯ - ç»Ÿä¸€æ„å›¾è·¯ç”±æ¶æ„
    
    è·¯ç”±æµç¨‹ï¼š
    1. æ„å›¾è¯†åˆ« â†’ åˆ†æç”¨æˆ·æ„å›¾å’Œä¸Šä¸‹æ–‡å……åˆ†æ€§
    2. è·¯ç”±å†³ç­– â†’ æ ¹æ®æ„å›¾ç±»å‹å’Œä¸Šä¸‹æ–‡å†³å®šæ‰§è¡Œè·¯å¾„
    3. æ‰§è¡Œ â†’ ç›´æ¥RAG/åŠ¨æ€æ”¶é›†ä¿¡æ¯/è®¡ç®—/å¯¹æ¯”
    """
    service = ConversationService(db)
    client = get_llm_client()
    
    # éªŒè¯å¯¹è¯å­˜åœ¨
    conv = service.get_conversation(conversation_id, str(user.id))
    if not conv:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Conversation not found"
        )
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒçš„äº¤äº’ä¼šè¯éœ€è¦ç»§ç»­
    active_session = db.query(InteractionSession).filter(
        InteractionSession.conversation_id == conversation_id,
        InteractionSession.user_id == user.id,
        InteractionSession.status == 'active'
    ).first()
    
    if active_session:
        # ç»§ç»­äº¤äº’ä¼šè¯
        return await _handle_interaction_answer(
            db, service, client, conversation_id, user, body.content, active_session
        )
    
    # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
    user_message = service.add_message(
        conversation_id=conversation_id,
        role="user",
        content=body.content
    )
    
    # ==================== ç»Ÿä¸€æ„å›¾è·¯ç”± ====================
    try:
        # Step 1: æ„å›¾è¯†åˆ«ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡å……åˆ†æ€§åˆ†æï¼‰
        intent_recognizer = get_intent_recognizer(client)
        history = [m.to_dict() for m in service.get_messages(conversation_id, limit=5)]
        
        intent_result = intent_recognizer.recognize(
            query=body.content,
            history=history,
            context={"scenario_id": conv.scenario_id}
        )
        
        logger.info(
            f"Unified routing: intent={intent_result.intent_type.value}, "
            f"action={intent_result.recommended_action.value}, "
            f"sufficient={intent_result.context_sufficiency.is_sufficient if intent_result.context_sufficiency else 'N/A'}"
        )
        
        # Step 2: æ ¹æ®æ¨èåŠ¨ä½œè·¯ç”±
        action = intent_result.recommended_action
        
        if action == ActionType.NEED_INFO:
            # éœ€è¦æ”¶é›†æ›´å¤šä¿¡æ¯ â†’ å¯åŠ¨åŠ¨æ€äº¤äº’
            return await _start_dynamic_interaction(
                db, service, client, conversation_id, user, 
                body.content, intent_result
            )
        
        elif action == ActionType.CALCULATE:
            # è®¡ç®—ç±» â†’ æ£€æŸ¥å‚æ•°åæ‰§è¡Œè®¡ç®—æˆ–æ”¶é›†ä¿¡æ¯
            return await _handle_calculation(
                db, service, client, conversation_id, user,
                body.content, intent_result
            )
        
        elif action == ActionType.COMPARE:
            # å¯¹æ¯”ç±» â†’ æ‰§è¡Œå¯¹æ¯”
            return await _handle_comparison(
                db, service, client, conversation_id, user,
                body.content, intent_result
            )
        
        else:
            # DIRECT_RAG â†’ ç›´æ¥æ£€ç´¢å›ç­”
            optimized_query = body.content
            user_context = {}
            
            if intent_result.context_sufficiency:
                optimized_query = intent_result.context_sufficiency.optimized_query or body.content
                user_context = intent_result.context_sufficiency.extracted_context or {}
            
            rag_result = await _generate_rag_response(
                query=optimized_query,
                conversation_id=conversation_id,
                scenario_id=conv.scenario_id,
                db=db,
                user_context=user_context,
                intent_result=intent_result
            )
            
            assistant_message = service.add_message(
                conversation_id=conversation_id,
                role="assistant",
                content=rag_result["answer"],
                sources=rag_result["sources"],
                model_used=rag_result.get("model", "qwen-max")
            )
            
            return MessageResponse(**assistant_message.to_dict())
            
    except Exception as e:
        logger.error(f"Unified routing error: {e}", exc_info=True)
        # å›é€€åˆ°åŸºæœ¬ RAG
        try:
            rag_result = await _generate_rag_response(
                query=body.content,
                conversation_id=conversation_id,
                scenario_id=conv.scenario_id,
                db=db
            )
            assistant_content = rag_result["answer"]
            sources = rag_result["sources"]
            model_used = rag_result.get("model", "qwen-max")
        except Exception as e2:
            logger.error(f"Fallback RAG also failed: {e2}")
            assistant_content = f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶é‡åˆ°é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚"
            sources = []
            model_used = "fallback"
        
        assistant_message = service.add_message(
            conversation_id=conversation_id,
            role="assistant",
            content=assistant_content,
            sources=sources,
            model_used=model_used
        )
        
        return MessageResponse(**assistant_message.to_dict())


# ==================== åŠ¨æ€äº¤äº’å¤„ç†å‡½æ•° ====================

async def _start_dynamic_interaction(
    db: Session,
    service: ConversationService,
    client: OpenAI,
    conversation_id: str,
    user: User,
    query: str,
    intent_result
) -> MessageResponse:
    """å¯åŠ¨åŠ¨æ€äº¤äº’æµç¨‹ - LLM åŠ¨æ€ç”Ÿæˆé—®é¢˜"""
    
    # åˆ›å»ºåŠ¨æ€äº¤äº’ä¼šè¯
    session_id = f"dyn_{uuid.uuid4().hex[:12]}"
    session = InteractionSession(
        session_id=session_id,
        conversation_id=conversation_id,
        flow_id=None,  # åŠ¨æ€æ¨¡å¼ä¸ç»‘å®šé™æ€æµç¨‹
        user_id=user.id,
        is_dynamic=True,
        original_query=query,
        intent_context={
            "intent_type": intent_result.intent_type.value,
            "entities": intent_result.entities,
            "scenario_ids": intent_result.scenario_ids,
            "missing_fields": intent_result.context_sufficiency.missing_fields if intent_result.context_sufficiency else []
        },
        collected_answers={},
        questions_asked=[]
    )
    db.add(session)
    db.commit()
    
    # ä½¿ç”¨åŠ¨æ€äº¤äº’æœåŠ¡ç”Ÿæˆç¬¬ä¸€ä¸ªé—®é¢˜
    interaction_service = get_dynamic_interaction_service(client)
    analysis = interaction_service.analyze_and_generate_question(
        intent_result=intent_result,
        collected_answers={},
        original_query=query
    )
    
    if analysis.can_proceed:
        # ä¸éœ€è¦æ”¶é›†ä¿¡æ¯ï¼Œç›´æ¥æ‰§è¡Œ
        db.delete(session)
        db.commit()
        
        rag_result = await _generate_rag_response(
            query=analysis.optimized_query or query,
            conversation_id=conversation_id,
            scenario_id=None,
            db=db,
            user_context=analysis.collected_context
        )
        
        assistant_message = service.add_message(
            conversation_id=conversation_id,
            role="assistant",
            content=rag_result["answer"],
            sources=rag_result["sources"],
            model_used=rag_result.get("model", "qwen-max")
        )
        
        return MessageResponse(**assistant_message.to_dict())
    
    # éœ€è¦æ”¶é›†ä¿¡æ¯ï¼Œè¿”å›é—®é¢˜
    if analysis.next_question:
        # è®°å½•é—®é¢˜
        question_dict = {
            "field_id": analysis.next_question.field_id,
            "question": analysis.next_question.question,
            "question_type": analysis.next_question.question_type.value,
            "options": [{"id": o.id, "label": o.label} for o in analysis.next_question.options],
            "placeholder": analysis.next_question.placeholder
        }
        session.questions_asked = [question_dict]
        flag_modified(session, "questions_asked")
        db.commit()
        
        # æ„å»ºå›å¤æ¶ˆæ¯
        assistant_content = _format_question_message(analysis.next_question)
        
        assistant_message = service.add_message(
            conversation_id=conversation_id,
            role="assistant",
            content=assistant_content,
            sources=[],
            model_used="dynamic_interaction"
        )
        
        response_dict = assistant_message.to_dict()
        response_dict["interaction"] = {
            "session_id": session_id,
            "is_dynamic": True,
            "question": question_dict,
            "progress": {"answered": 0, "total": len(analysis.missing_fields) or 1}
        }
        
        return MessageResponse(**response_dict)
    
    # æ²¡æœ‰é—®é¢˜ç”Ÿæˆï¼Œå›é€€åˆ° RAG
    db.delete(session)
    db.commit()
    
    rag_result = await _generate_rag_response(
        query=query,
        conversation_id=conversation_id,
        scenario_id=None,
        db=db
    )
    
    assistant_message = service.add_message(
        conversation_id=conversation_id,
        role="assistant",
        content=rag_result["answer"],
        sources=rag_result["sources"],
        model_used=rag_result.get("model", "qwen-max")
    )
    
    return MessageResponse(**assistant_message.to_dict())


async def _handle_interaction_answer(
    db: Session,
    service: ConversationService,
    client: OpenAI,
    conversation_id: str,
    user: User,
    answer: str,
    session: InteractionSession
) -> MessageResponse:
    """å¤„ç†åŠ¨æ€äº¤äº’ä¸­çš„ç”¨æˆ·å›ç­”"""
    
    # ä¿å­˜ç”¨æˆ·å›ç­”æ¶ˆæ¯
    user_message = service.add_message(
        conversation_id=conversation_id,
        role="user",
        content=answer
    )
    
    # è·å–æœ€åä¸€ä¸ªé—®é¢˜
    questions_asked = session.questions_asked or []
    if questions_asked:
        last_question = questions_asked[-1]
        field_id = last_question.get("field_id", f"answer_{len(questions_asked)}")
        
        # ä¿å­˜ç­”æ¡ˆ
        answers = dict(session.collected_answers or {})
        answers[field_id] = answer
        session.collected_answers = answers
        session.current_step = len(answers)
        flag_modified(session, "collected_answers")
    
    # é‡æ–°åˆ†ææ˜¯å¦éœ€è¦æ›´å¤šä¿¡æ¯
    intent_recognizer = get_intent_recognizer(client)
    
    # æ„å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡
    enhanced_context = session.intent_context or {}
    enhanced_context["collected_answers"] = session.collected_answers
    
    # é‡æ–°è¯†åˆ«æ„å›¾ï¼ˆåŒ…å«å·²æ”¶é›†çš„ä¿¡æ¯ï¼‰
    from ..services.intent_recognizer import IntentResult as IR
    intent_result = IR(
        intent_type=IntentType(enhanced_context.get("intent_type", "general")),
        confidence=0.8,
        entities=enhanced_context.get("entities", {}),
        scenario_ids=enhanced_context.get("scenario_ids", [])
    )
    
    # ä½¿ç”¨åŠ¨æ€äº¤äº’æœåŠ¡åˆ†æ
    interaction_service = get_dynamic_interaction_service(client)
    analysis = interaction_service.analyze_and_generate_question(
        intent_result=intent_result,
        collected_answers=session.collected_answers,
        original_query=session.original_query or "",
        history=[m.to_dict() for m in service.get_messages(conversation_id, limit=5)]
    )
    
    if analysis.can_proceed:
        # ä¿¡æ¯æ”¶é›†å®Œæˆï¼Œæ‰§è¡Œæ“ä½œ
        session.status = 'completed'
        session.optimized_query = analysis.optimized_query
        db.commit()
        
        # æ ¹æ®æ„å›¾ç±»å‹æ‰§è¡Œä¸åŒæ“ä½œ
        intent_type_str = enhanced_context.get("intent_type", "general")
        
        if intent_type_str == "calculation":
            # æ‰§è¡Œè®¡ç®—
            result_content = await _execute_calculation(
                client, session.collected_answers, session.original_query
            )
        else:
            # æ‰§è¡Œ RAG æ£€ç´¢
            rag_result = await _generate_rag_response(
                query=analysis.optimized_query or session.original_query,
                conversation_id=conversation_id,
                scenario_id=None,
                db=db,
                user_context={**analysis.collected_context, **session.collected_answers}
            )
            result_content = rag_result["answer"]
        
        # æ·»åŠ æ”¶é›†ä¿¡æ¯æ‘˜è¦
        info_summary = interaction_service.format_collected_info_for_display(
            session.collected_answers,
            [DynamicQuestion(
                field_id=q.get("field_id", ""),
                question=q.get("question", ""),
                question_type=QuestionType(q.get("question_type", "single")),
                options=[]
            ) for q in questions_asked]
        )
        
        if info_summary:
            result_content = f"{info_summary}\n\n---\n\n{result_content}"
        
        assistant_message = service.add_message(
            conversation_id=conversation_id,
            role="assistant",
            content=result_content,
            sources=[],
            model_used="dynamic_interaction"
        )
        
        return MessageResponse(**assistant_message.to_dict())
    
    # éœ€è¦ç»§ç»­æ”¶é›†ä¿¡æ¯
    if analysis.next_question:
        # è®°å½•æ–°é—®é¢˜
        question_dict = {
            "field_id": analysis.next_question.field_id,
            "question": analysis.next_question.question,
            "question_type": analysis.next_question.question_type.value,
            "options": [{"id": o.id, "label": o.label} for o in analysis.next_question.options],
            "placeholder": analysis.next_question.placeholder
        }
        questions_asked.append(question_dict)
        session.questions_asked = questions_asked
        flag_modified(session, "questions_asked")
        db.commit()
        
        assistant_content = _format_question_message(analysis.next_question)
        
        assistant_message = service.add_message(
            conversation_id=conversation_id,
            role="assistant",
            content=assistant_content,
            sources=[],
            model_used="dynamic_interaction"
        )
        
        response_dict = assistant_message.to_dict()
        response_dict["interaction"] = {
            "session_id": session.session_id,
            "is_dynamic": True,
            "question": question_dict,
            "progress": {
                "answered": len(session.collected_answers or {}),
                "total": len(analysis.missing_fields) + len(session.collected_answers or {}) or 1
            }
        }
        
        return MessageResponse(**response_dict)
    
    # æ²¡æœ‰æ›´å¤šé—®é¢˜ï¼Œå®Œæˆä¼šè¯
    session.status = 'completed'
    db.commit()
    
    rag_result = await _generate_rag_response(
        query=session.original_query or answer,
        conversation_id=conversation_id,
        scenario_id=None,
        db=db,
        user_context=session.collected_answers
    )
    
    assistant_message = service.add_message(
        conversation_id=conversation_id,
        role="assistant",
        content=rag_result["answer"],
        sources=rag_result["sources"],
        model_used=rag_result.get("model", "qwen-max")
    )
    
    return MessageResponse(**assistant_message.to_dict())


def _format_question_message(question: DynamicQuestion) -> str:
    """æ ¼å¼åŒ–é—®é¢˜æ¶ˆæ¯"""
    message = question.question
    
    if question.question_type == QuestionType.SINGLE and question.options:
        options_text = "\n".join([f"  â€¢ {opt.label}" for opt in question.options])
        message += f"\n\n{options_text}"
    elif question.question_type == QuestionType.MULTI and question.options:
        options_text = "\n".join([f"  â˜ {opt.label}" for opt in question.options])
        message += f"\n\nï¼ˆå¯å¤šé€‰ï¼‰\n{options_text}"
    elif question.question_type == QuestionType.INPUT and question.placeholder:
        message += f"\n\nï¼ˆè¯·è¾“å…¥ï¼Œå¦‚ï¼š{question.placeholder}ï¼‰"
    
    return message


async def _handle_calculation(
    db: Session,
    service: ConversationService,
    client: OpenAI,
    conversation_id: str,
    user: User,
    query: str,
    intent_result
) -> MessageResponse:
    """å¤„ç†è®¡ç®—ç±»æ„å›¾"""
    entities = intent_result.entities
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å‚æ•°
    has_capacity = "capacity" in entities
    has_power = "power" in entities
    has_count = "device_count" in entities
    
    if not (has_capacity or has_power or has_count):
        # å‚æ•°ä¸è¶³ï¼Œå¯åŠ¨åŠ¨æ€äº¤äº’
        return await _start_dynamic_interaction(
            db, service, client, conversation_id, user, query, intent_result
        )
    
    # æ‰§è¡Œè®¡ç®—
    result_content = await _execute_calculation(client, entities, query)
    
    assistant_message = service.add_message(
        conversation_id=conversation_id,
        role="assistant",
        content=result_content,
        sources=[],
        model_used="calculation"
    )
    
    return MessageResponse(**assistant_message.to_dict())


async def _execute_calculation(
    client: OpenAI,
    params: dict,
    query: str
) -> str:
    """æ‰§è¡Œè®¡ç®—"""
    prompt = f"""åŸºäºä»¥ä¸‹å‚æ•°è¿›è¡Œè®¡ç®—ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{query}
å‚æ•°ï¼š{params}

è¯·è¿›è¡Œç›¸å…³è®¡ç®—ï¼ˆå¦‚è®¾å¤‡æ•°é‡ã€äº§èƒ½ã€ROIç­‰ï¼‰ï¼Œå¹¶ç»™å‡ºè¯¦ç»†çš„è®¡ç®—è¿‡ç¨‹å’Œç»“æœã€‚
ä½¿ç”¨è¡¨æ ¼å±•ç¤ºå…³é”®æ•°æ®ã€‚"""

    try:
        response = client.chat.completions.create(
            model="qwen-max",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=1024
        )
        return response.choices[0].message.content
    except Exception as e:
        logger.error(f"Calculation failed: {e}")
        return f"è®¡ç®—è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼š{str(e)}"


async def _handle_comparison(
    db: Session,
    service: ConversationService,
    client: OpenAI,
    conversation_id: str,
    user: User,
    query: str,
    intent_result
) -> MessageResponse:
    """å¤„ç†å¯¹æ¯”ç±»æ„å›¾"""
    # å¯¹æ¯”ç±»ç›´æ¥ä½¿ç”¨ RAGï¼Œä½†æç¤ºä½¿ç”¨è¡¨æ ¼æ ¼å¼
    optimized_query = query
    if intent_result.context_sufficiency:
        optimized_query = intent_result.context_sufficiency.optimized_query or query
    
    rag_result = await _generate_rag_response(
        query=optimized_query,
        conversation_id=conversation_id,
        scenario_id=None,
        db=db,
        user_context=intent_result.context_sufficiency.extracted_context if intent_result.context_sufficiency else {},
        intent_result=intent_result,
        force_table_format=True
    )
    
    assistant_message = service.add_message(
        conversation_id=conversation_id,
        role="assistant",
        content=rag_result["answer"],
        sources=rag_result["sources"],
        model_used=rag_result.get("model", "qwen-max")
    )
    
    return MessageResponse(**assistant_message.to_dict())


async def _generate_rag_response(
    query: str,
    conversation_id: str,
    scenario_id: Optional[str] = None,
    db: Session = None,
    user_context: dict = None,
    intent_result = None,
    force_table_format: bool = False
) -> dict:
    """
    Generate RAG response using retrieval and LLM
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢ï¼ˆå¯èƒ½æ˜¯ LLM ä¼˜åŒ–åçš„ï¼‰
        conversation_id: å¯¹è¯ ID
        scenario_id: åœºæ™¯ ID
        db: æ•°æ®åº“ä¼šè¯
        user_context: ç”¨æˆ·æ„å›¾ä¸Šä¸‹æ–‡ï¼ˆLLM æå–çš„ä¸»é¢˜ã€å…³é”®è¯ç­‰ï¼‰
        intent_result: é¢„è®¡ç®—çš„æ„å›¾è¯†åˆ«ç»“æœï¼ˆå¯é€‰ï¼Œé¿å…é‡å¤è®¡ç®—ï¼‰
        force_table_format: å¼ºåˆ¶ä½¿ç”¨è¡¨æ ¼æ ¼å¼ï¼ˆå¯¹æ¯”ç±»ï¼‰
    
    Returns:
        dict with keys: answer, sources, model
    """
    user_context = user_context or {}
    
    # 1. æ„å›¾è¯†åˆ«ï¼ˆå¦‚æœæ²¡æœ‰é¢„è®¡ç®—ï¼‰
    client = get_llm_client()
    
    if intent_result is None:
        intent_recognizer = get_intent_recognizer(client)
        try:
            intent_result = intent_recognizer.recognize(query)
            logger.info(f"Intent recognized: {intent_result.intent_type}")
        except Exception as e:
            logger.warning(f"Intent recognition failed: {e}")
            intent_result = None
    
    # 2. å¢å¼ºæ£€ç´¢æŸ¥è¯¢ - ç»“åˆç”¨æˆ·ä¸Šä¸‹æ–‡
    enhanced_query = query
    if user_context:
        topic = user_context.get("topic", "")
        keywords = user_context.get("keywords", [])
        product = user_context.get("product", "")
        
        # å¦‚æœæœ‰æå–çš„ä¸»é¢˜/å…³é”®è¯ï¼Œæ·»åŠ åˆ°æŸ¥è¯¢ä¸­ä»¥æé«˜æ£€ç´¢ç²¾åº¦
        context_terms = [topic] if topic else []
        if isinstance(keywords, list):
            context_terms.extend(keywords)
        if product:
            context_terms.append(product)
        
        if context_terms:
            # å°†ä¸Šä¸‹æ–‡å…³é”®è¯åŠ å…¥æŸ¥è¯¢ï¼Œé¿å…é‡å¤
            unique_terms = [t for t in context_terms if t and t.lower() not in query.lower()]
            if unique_terms:
                enhanced_query = f"{query} {' '.join(unique_terms)}"
                logger.info(f"Enhanced query with context: {enhanced_query}")
    
    # 3. æ£€ç´¢çŸ¥è¯†
    try:
        hits = search(enhanced_query, top_k=5, intent_result=intent_result)
        logger.info(f"Retrieved {len(hits)} hits")
    except Exception as e:
        logger.error(f"Retrieval error: {e}")
        hits = []
    
    # 3. æ„å»ºä¸Šä¸‹æ–‡
    sources = []
    context_parts = []
    
    for hit in hits[:5]:
        title = hit.get("title", "æœªçŸ¥æ ‡é¢˜")
        summary = hit.get("summary", "")
        body = hit.get("body", "")[:1500]
        source_file = hit.get("source_file", "")
        
        context_parts.append(f"ã€æ¥æº: {source_file}ã€‘\næ ‡é¢˜: {title}\næ‘˜è¦: {summary}\nè¯¦æƒ…: {body}")
        
        sources.append({
            "id": hit.get("id"),
            "title": title,
            "type": hit.get("ku_type", "core"),
            "source_file": source_file,
            "score": hit.get("score", 0)
        })
    
    # 4. æ„å»º Prompt
    format_instruction = ""
    if force_table_format:
        format_instruction = """
**é‡è¦ï¼šæœ¬æ¬¡å›ç­”å¿…é¡»ä½¿ç”¨ Markdown è¡¨æ ¼æ ¼å¼**
è¯·å°†å¯¹æ¯”ä¿¡æ¯æ•´ç†æˆæ¸…æ™°çš„è¡¨æ ¼ï¼ŒåŒ…å«æ‰€æœ‰ç›¸å…³å‚æ•°çš„å¯¹æ¯”ã€‚

è¡¨æ ¼æ ¼å¼ç¤ºä¾‹ï¼š
| å¯¹æ¯”é¡¹ | äº§å“A | äº§å“B |
|--------|-------|-------|
| ç²¾åº¦   | 0.01mm | 0.02mm |
| äº§èƒ½   | 5000ç‰‡/h | 3500ç‰‡/h |
"""
    else:
        format_instruction = """
æ ¼å¼åŒ–è¾“å‡ºè¦æ±‚ï¼ˆä½¿ç”¨ Markdownï¼‰ï¼š
- **å¯¹æ¯”ç±»é—®é¢˜**ï¼šä½¿ç”¨ Markdown è¡¨æ ¼å±•ç¤º
- **å¤šæ­¥éª¤/æµç¨‹**ï¼šä½¿ç”¨æœ‰åºåˆ—è¡¨ 1. 2. 3.
- **å¤šè¦ç‚¹**ï¼šä½¿ç”¨æ— åºåˆ—è¡¨ - æˆ– *
- **ä»£ç /é…ç½®**ï¼šä½¿ç”¨ä»£ç å—
- **é‡ç‚¹å†…å®¹**ï¼šä½¿ç”¨ **åŠ ç²—** æˆ– `è¡Œå†…ä»£ç `
"""
    
    # æ£€æµ‹æœç´¢ç»“æœæ˜¯å¦çœŸæ­£åŒ¹é…ç”¨æˆ·éœ€æ±‚
    has_good_matches = False
    low_quality_threshold = 0.35  # ä½äºæ­¤åˆ†æ•°è®¤ä¸ºä¸åŒ¹é…
    
    if context_parts:
        # æ£€æŸ¥å¹³å‡åˆ†æ•° - å¦‚æœæ‰€æœ‰ç»“æœåˆ†æ•°éƒ½å¾ˆä½ï¼Œè¯´æ˜æ²¡æœ‰å¥½çš„åŒ¹é…
        avg_score = sum(s.get("score", 0) for s in sources) / len(sources) if sources else 0
        max_score = max((s.get("score", 0) for s in sources), default=0)
        has_good_matches = max_score > low_quality_threshold
        
        logger.info(f"Search quality: max_score={max_score:.3f}, avg_score={avg_score:.3f}, has_good_matches={has_good_matches}")
    
    if context_parts and has_good_matches:
        context = "\n\n---\n\n".join(context_parts)
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„çŸ¥è¯†å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

{context}

å›ç­”è¦æ±‚ï¼š
1. åŸºäºä¸Šè¿°å†…å®¹å‡†ç¡®å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
2. åœ¨é€‚å½“ä½ç½®å¼•ç”¨æ¥æºï¼Œæ ¼å¼å¦‚ã€æ¥æº: xxxã€‘
3. å¦‚æœæ£€ç´¢å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·å¦‚å®è¯´æ˜
4. ä½¿ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€
{format_instruction}"""
    elif context_parts and not has_good_matches:
        # æœ‰ç»“æœä½†ä¸åŒ¹é… - è¿”å›ç›¸å…³å†…å®¹ä½†é‚€è¯·è´¡çŒ®
        context = "\n\n---\n\n".join(context_parts)
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åŠ©æ‰‹ã€‚ç”¨æˆ·æŸ¥è¯¢çš„å†…å®¹åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›´æ¥åŒ¹é…çš„èµ„æ–™ï¼Œä½†æ‰¾åˆ°äº†ä¸€äº›å¯èƒ½ç›¸å…³çš„å†…å®¹ã€‚

{context}

å›ç­”è¦æ±‚ï¼š
1. é¦–å…ˆæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ï¼šç›®å‰çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸"{query}"ç›´æ¥ç›¸å…³çš„èµ„æ–™
2. å¦‚æœä¸Šè¿°ç›¸å…³å†…å®¹å¯¹ç”¨æˆ·æœ‰å‚è€ƒä»·å€¼ï¼Œå¯ä»¥ç®€è¦æåŠ
3. **é‡ç‚¹**ï¼šé‚€è¯·ç”¨æˆ·è´¡çŒ®ç›¸å…³ææ–™ï¼Œä¾‹å¦‚ï¼š
   "å¦‚æœæ‚¨æ‰‹ä¸Šæœ‰ç›¸å…³èµ„æ–™ï¼Œæ¬¢è¿é€šè¿‡ä¸Šä¼ åŠŸèƒ½åˆ†äº«ç»™æˆ‘ä»¬ï¼Œå¸®åŠ©ä¸°å¯ŒçŸ¥è¯†åº“ï¼"
4. ä¿æŒå‹å¥½å’Œä¸“ä¸šçš„è¯­æ°”
{format_instruction}"""
    else:
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åŠ©æ‰‹ã€‚å½“å‰çŸ¥è¯†åº“æœªæ‰¾åˆ°ä¸ç”¨æˆ·æŸ¥è¯¢ç›¸å…³çš„å†…å®¹ã€‚

å›ç­”è¦æ±‚ï¼š
1. å‘ŠçŸ¥ç”¨æˆ·ï¼šç›®å‰çŸ¥è¯†åº“ä¸­æš‚æ— ä¸"{query}"ç›¸å…³çš„èµ„æ–™
2. **é‡è¦**ï¼šç§¯æé‚€è¯·ç”¨æˆ·è´¡çŒ®ææ–™ï¼š
   "å¦‚æœæ‚¨æœ‰ç›¸å…³èµ„æ–™ï¼Œæ¬¢è¿é€šè¿‡ä¸Šä¼ åŠŸèƒ½åˆ†äº«ç»™æˆ‘ä»¬ï¼æ‚¨çš„è´¡çŒ®å°†å¸®åŠ©æˆ‘ä»¬å®Œå–„çŸ¥è¯†åº“ï¼Œä¹Ÿèƒ½å¸®åŠ©åˆ°å…¶ä»–åŒäº‹ã€‚"
3. å¯ä»¥è¯¢é—®ç”¨æˆ·æ˜¯å¦éœ€è¦å…¶ä»–å¸®åŠ©
4. ä¿æŒå‹å¥½å’Œä¸“ä¸šçš„è¯­æ°”"""
    
    # 5. è°ƒç”¨ LLM ç”Ÿæˆå›ç­”
    try:
        response = client.chat.completions.create(
            model="qwen-max",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query}
            ],
            temperature=0.7,
            max_tokens=2048
        )
        answer = response.choices[0].message.content
        model = response.model if hasattr(response, 'model') else "qwen-max"
    except Exception as e:
        logger.error(f"LLM call failed: {e}")
        if hits:
            # å›é€€ï¼šç›´æ¥è¿”å›æ£€ç´¢ç»“æœæ‘˜è¦
            summaries = [f"â€¢ {h.get('title', '')}: {h.get('summary', '')[:100]}" for h in hits[:3]]
            answer = f"æŠ±æ­‰ï¼ŒAI å›ç­”ç”Ÿæˆé‡åˆ°é—®é¢˜ï¼Œä»¥ä¸‹æ˜¯æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹ï¼š\n\n" + "\n\n".join(summaries)
        else:
            answer = "æŠ±æ­‰ï¼Œå½“å‰æ— æ³•ç”Ÿæˆå›ç­”ï¼Œè¯·ç¨åé‡è¯•ã€‚"
        model = "fallback"
    
    return {
        "answer": answer,
        "sources": sources,
        "model": model
    }


@router.put("/{conversation_id}/messages/{message_id}/feedback")
async def update_feedback(
    conversation_id: str,
    message_id: str,
    body: FeedbackRequest,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """æ›´æ–°æ¶ˆæ¯åé¦ˆ"""
    service = ConversationService(db)
    
    # éªŒè¯å¯¹è¯å±äºç”¨æˆ·
    conv = service.get_conversation(conversation_id, str(user.id))
    if not conv:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Conversation not found"
        )
    
    message = service.update_message_feedback(message_id, body.feedback, body.feedback_text)
    
    if not message:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Message not found"
        )
    
    return {"message": "Feedback updated"}


# ==================== Share Endpoints ====================

# ==================== Export Endpoint ====================

class ExportResponse(BaseModel):
    content: str
    filename: str


@router.get("/{conversation_id}/export", response_model=ExportResponse)
async def export_conversation(
    conversation_id: str,
    format: str = "markdown",  # markdown or json
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """å¯¼å‡ºå¯¹è¯"""
    service = ConversationService(db)
    
    # éªŒè¯å¯¹è¯å±äºç”¨æˆ·
    conv = service.get_conversation(conversation_id, str(user.id))
    if not conv:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Conversation not found"
        )
    
    messages = service.get_messages(conversation_id, limit=1000)
    
    if format == "json":
        import json
        content = json.dumps({
            "conversation": conv.to_dict(),
            "messages": [m.to_dict() for m in messages]
        }, ensure_ascii=False, indent=2)
        filename = f"conversation-{conversation_id}.json"
    else:
        # Markdown format
        lines = [
            f"# {conv.title or 'å¯¹è¯å¯¼å‡º'}",
            "",
            f"å¯¼å‡ºæ—¶é—´: {conv.created_at}",
            "",
            "---",
            ""
        ]
        
        for msg in messages:
            role = "**ç”¨æˆ·**" if msg.role == "user" else "**åŠ©æ‰‹**"
            lines.append(role)
            lines.append("")
            lines.append(msg.content)
            lines.append("")
            
            # Add sources if available
            if msg.sources:
                lines.append("ğŸ“ æ¥æºï¼š")
                for src in msg.sources:
                    title = src.get("title", "æœªçŸ¥") if isinstance(src, dict) else str(src)
                    lines.append(f"- {title}")
                lines.append("")
            
            lines.append("---")
            lines.append("")
        
        content = "\n".join(lines)
        filename = f"conversation-{conversation_id}.md"
    
    return ExportResponse(content=content, filename=filename)


# ==================== Share Endpoints ====================

@router.post("/{conversation_id}/share")
async def create_share(
    conversation_id: str,
    body: CreateShareRequest,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """åˆ›å»ºåˆ†äº«é“¾æ¥"""
    service = ConversationService(db)
    
    # éªŒè¯å¯¹è¯å±äºç”¨æˆ·
    conv = service.get_conversation(conversation_id, str(user.id))
    if not conv:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Conversation not found"
        )
    
    share = service.create_share(
        conversation_id=conversation_id,
        user_id=user.id,
        allow_copy=body.allow_copy,
        expires_in_days=body.expires_in_days
    )
    
    return {
        "share_token": share.share_token,
        "share_url": f"/share/{share.share_token}",
        "expires_at": share.expires_at.isoformat() if share.expires_at else None
    }


@router.delete("/{conversation_id}/share")
async def delete_share(
    conversation_id: str,
    user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """å–æ¶ˆåˆ†äº«"""
    service = ConversationService(db)
    success = service.delete_share(conversation_id, user.id)
    
    if not success:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Share not found"
        )
    
    return {"message": "Share deleted"}


# ==================== Public Share Access ====================

@router.get("/share/{token}")
async def get_shared_conversation(
    token: str,
    db: Session = Depends(get_db)
):
    """è·å–åˆ†äº«çš„å¯¹è¯ï¼ˆå…¬å¼€è®¿é—®ï¼‰"""
    service = ConversationService(db)
    result = service.get_shared_conversation(token)
    
    if not result:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Share not found or expired"
        )
    
    return result

