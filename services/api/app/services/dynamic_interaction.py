"""
åŠ¨æ€äº¤äº’æµç¨‹æœåŠ¡
ä½¿ç”¨ LLM åŠ¨æ€ç”Ÿæˆé—®é¢˜ï¼Œè€Œéé™æ€é¢„å®šä¹‰é—®é¢˜
"""
import json
import logging
from typing import Optional, Dict, Any, List
from dataclasses import dataclass, field
from enum import Enum
from openai import OpenAI

from .intent_recognizer import IntentResult, IntentType, ActionType

logger = logging.getLogger(__name__)


class QuestionType(str, Enum):
    """é—®é¢˜ç±»å‹"""
    SINGLE = "single"      # å•é€‰
    MULTI = "multi"        # å¤šé€‰
    INPUT = "input"        # è¾“å…¥
    CONFIRM = "confirm"    # ç¡®è®¤


@dataclass
class QuestionOption:
    """é—®é¢˜é€‰é¡¹"""
    id: str
    label: str
    icon: Optional[str] = None


@dataclass
class DynamicQuestion:
    """åŠ¨æ€ç”Ÿæˆçš„é—®é¢˜"""
    field_id: str                    # å­—æ®µIDï¼ˆå¦‚ industry, productï¼‰
    question: str                    # é—®é¢˜æ–‡æœ¬
    question_type: QuestionType      # é—®é¢˜ç±»å‹
    options: List[QuestionOption] = field(default_factory=list)
    placeholder: str = ""            # è¾“å…¥å ä½ç¬¦
    required: bool = True
    reason: str = ""                 # ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªä¿¡æ¯


@dataclass
class MissingInfoAnalysis:
    """ç¼ºå¤±ä¿¡æ¯åˆ†æç»“æœ"""
    can_proceed: bool                # æ˜¯å¦å¯ä»¥ç›´æ¥æ‰§è¡Œ
    missing_fields: List[Dict[str, Any]] = field(default_factory=list)
    next_question: Optional[DynamicQuestion] = None
    optimized_query: str = ""        # ä¼˜åŒ–åçš„æŸ¥è¯¢
    collected_context: Dict[str, Any] = field(default_factory=dict)


class DynamicInteractionService:
    """
    åŠ¨æ€äº¤äº’æµç¨‹æœåŠ¡
    
    æ ¸å¿ƒç†å¿µï¼š
    - ä¸ä½¿ç”¨é¢„å®šä¹‰çš„é™æ€é—®é¢˜åˆ—è¡¨
    - LLM æ ¹æ®å½“å‰ä¸Šä¸‹æ–‡åŠ¨æ€å†³å®šéœ€è¦é—®ä»€ä¹ˆ
    - æ¯æ¬¡åªç”Ÿæˆä¸€ä¸ªæœ€å…³é”®çš„é—®é¢˜
    - é—®é¢˜å†…å®¹ä¸ç”¨æˆ·å·²æä¾›çš„ä¿¡æ¯ç›¸å…³è”
    """
    
    def __init__(self, llm_client: Optional[OpenAI] = None):
        self.llm_client = llm_client
    
    def analyze_and_generate_question(
        self,
        intent_result: IntentResult,
        collected_answers: Dict[str, Any] = None,
        original_query: str = "",
        history: List[Dict] = None
    ) -> MissingInfoAnalysis:
        """
        åˆ†æç¼ºå¤±ä¿¡æ¯å¹¶åŠ¨æ€ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜
        
        Args:
            intent_result: æ„å›¾è¯†åˆ«ç»“æœ
            collected_answers: å·²æ”¶é›†çš„ç­”æ¡ˆ
            original_query: ç”¨æˆ·åŸå§‹é—®é¢˜
            history: å¯¹è¯å†å²
        
        Returns:
            MissingInfoAnalysis: åˆ†æç»“æœï¼ŒåŒ…å«ä¸‹ä¸€ä¸ªé—®é¢˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
        """
        collected_answers = collected_answers or {}
        history = history or []
        
        if not self.llm_client:
            # æ—  LLM æ—¶å›é€€åˆ°åŸºäºè§„åˆ™çš„ç®€å•é€»è¾‘
            return self._rule_based_analysis(intent_result, collected_answers)
        
        return self._llm_based_analysis(
            intent_result, collected_answers, original_query, history
        )
    
    def _llm_based_analysis(
        self,
        intent_result: IntentResult,
        collected_answers: Dict[str, Any],
        original_query: str,
        history: List[Dict]
    ) -> MissingInfoAnalysis:
        """ä½¿ç”¨ LLM åˆ†æå¹¶ç”Ÿæˆé—®é¢˜"""
        
        # æ„å»ºå·²çŸ¥ä¿¡æ¯
        known_info = {
            "intent": intent_result.intent_type.value,
            "entities": intent_result.entities,
            "scenarios": intent_result.scenario_ids,
            "keywords": intent_result.matched_keywords,
            "collected_answers": collected_answers,
        }
        
        # å¦‚æœæœ‰ä¸Šä¸‹æ–‡å……åˆ†æ€§åˆ†æç»“æœï¼Œä¹ŸåŒ…å«è¿›æ¥
        if intent_result.context_sufficiency:
            known_info["extracted_context"] = intent_result.context_sufficiency.extracted_context
        
        known_info_text = json.dumps(known_info, ensure_ascii=False, indent=2)
        
        # æ„å»ºå¯¹è¯å†å²
        history_text = "æ— "
        if history:
            history_lines = [
                f"{'ç”¨æˆ·' if m.get('role') == 'user' else 'åŠ©æ‰‹'}: {m.get('content', '')[:100]}"
                for m in history[-5:]
            ]
            history_text = "\n".join(history_lines)
        
        prompt = f"""åˆ†æç”¨æˆ·æ„å›¾ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ”¶é›†æ›´å¤šä¿¡æ¯ï¼Œå¦‚æœéœ€è¦åˆ™ç”Ÿæˆä¸€ä¸ªé—®é¢˜ã€‚

ç”¨æˆ·åŸå§‹é—®é¢˜: {original_query}
æ„å›¾ç±»å‹: {intent_result.intent_type.value}
å·²çŸ¥ä¿¡æ¯:
{known_info_text}

å¯¹è¯å†å²:
{history_text}

ä»»åŠ¡è¯´æ˜:
1. åˆ¤æ–­æ˜¯å¦å·²ç»æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥æ‰§è¡Œç”¨æˆ·è¯·æ±‚
2. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œç¡®å®šæœ€å…³é”®çš„ç¼ºå¤±ä¿¡æ¯æ˜¯ä»€ä¹ˆ
3. ç”Ÿæˆä¸€ä¸ªä¸ä¸Šä¸‹æ–‡ç›¸å…³çš„é—®é¢˜ï¼ˆä¸æ˜¯é€šç”¨é—®é¢˜ï¼‰

ç”Ÿæˆé—®é¢˜çš„è§„åˆ™:
- é—®é¢˜è¦ä¸ç”¨æˆ·åŸå§‹é—®é¢˜ç›¸å…³è”
- å¦‚æœç”¨æˆ·é—®"å·¥æ§å®‰å…¨æ¡ˆä¾‹"ï¼Œä¸è¦é—®"è¯·é€‰æ‹©è¡Œä¸š"ï¼ˆå› ä¸ºå·²ç»è¯´äº†å·¥æ§ï¼‰
- é—®é¢˜åº”è¯¥ç®€æ´æ˜äº†
- æä¾›çš„é€‰é¡¹åº”è¯¥ä¸ç”¨æˆ·ä¸Šä¸‹æ–‡ç›¸å…³
- æ¯æ¬¡åªé—®ä¸€ä¸ªæœ€å…³é”®çš„é—®é¢˜

è¯·ä»¥ JSON æ ¼å¼è¿”å›:
{{
    "can_proceed": true/false,
    "reason": "åˆ¤æ–­ç†ç”±",
    "missing_fields": [
        {{"field": "å­—æ®µå", "importance": "required/optional", "description": "ä¸ºä»€ä¹ˆéœ€è¦"}}
    ],
    "optimized_query": "å¦‚æœå¯ä»¥ç»§ç»­ï¼Œç”¨äºæ£€ç´¢çš„ä¼˜åŒ–æŸ¥è¯¢",
    "next_question": {{
        "field_id": "å­—æ®µID",
        "question": "é—®é¢˜æ–‡æœ¬ï¼ˆä¸ä¸Šä¸‹æ–‡ç›¸å…³ï¼‰",
        "question_type": "single/multi/input/confirm",
        "options": [
            {{"id": "é€‰é¡¹ID", "label": "é€‰é¡¹æ–‡æœ¬"}}
        ],
        "placeholder": "è¾“å…¥ç±»å‹çš„å ä½ç¬¦",
        "reason": "ä¸ºä»€ä¹ˆé—®è¿™ä¸ªé—®é¢˜"
    }}
}}

å¦‚æœ can_proceed=trueï¼Œnext_question å¯ä»¥ä¸º nullã€‚
åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        try:
            response = self.llm_client.chat.completions.create(
                model="qwen-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=1024,
            )
            
            content = response.choices[0].message.content.strip()
            logger.info(f"Dynamic question generation: {content[:300]}...")
            
            import re
            json_match = re.search(r"\{[\s\S]*\}", content)
            if json_match:
                result = json.loads(json_match.group())
                
                can_proceed = result.get("can_proceed", True)
                missing_fields = result.get("missing_fields", [])
                optimized_query = result.get("optimized_query", original_query)
                
                next_question = None
                if not can_proceed and result.get("next_question"):
                    q = result["next_question"]
                    options = [
                        QuestionOption(id=opt["id"], label=opt["label"], icon=opt.get("icon"))
                        for opt in q.get("options", [])
                    ]
                    next_question = DynamicQuestion(
                        field_id=q.get("field_id", "unknown"),
                        question=q.get("question", ""),
                        question_type=QuestionType(q.get("question_type", "single")),
                        options=options,
                        placeholder=q.get("placeholder", ""),
                        reason=q.get("reason", "")
                    )
                
                return MissingInfoAnalysis(
                    can_proceed=can_proceed,
                    missing_fields=missing_fields,
                    next_question=next_question,
                    optimized_query=optimized_query,
                    collected_context=intent_result.context_sufficiency.extracted_context 
                        if intent_result.context_sufficiency else {}
                )
                
        except Exception as e:
            logger.warning(f"LLM dynamic question generation failed: {e}")
        
        # å›é€€ï¼šå…è®¸ç»§ç»­
        return MissingInfoAnalysis(
            can_proceed=True,
            optimized_query=original_query
        )
    
    def _rule_based_analysis(
        self,
        intent_result: IntentResult,
        collected_answers: Dict[str, Any]
    ) -> MissingInfoAnalysis:
        """åŸºäºè§„åˆ™çš„ç®€å•åˆ†æï¼ˆæ—  LLM æ—¶ä½¿ç”¨ï¼‰"""
        
        intent = intent_result.intent_type
        entities = intent_result.entities
        
        # æ¡ˆä¾‹æŸ¥è¯¢ï¼šéœ€è¦è¡Œä¸šæˆ–ä¸»é¢˜
        if intent == IntentType.CASE_STUDY:
            if not intent_result.scenario_ids and "industry" not in collected_answers:
                return MissingInfoAnalysis(
                    can_proceed=False,
                    missing_fields=[{"field": "industry", "importance": "required"}],
                    next_question=DynamicQuestion(
                        field_id="industry",
                        question="æ‚¨æƒ³æŸ¥æ‰¾å“ªä¸ªè¡Œä¸šçš„æ¡ˆä¾‹ï¼Ÿ",
                        question_type=QuestionType.SINGLE,
                        options=[
                            QuestionOption(id="automotive", label="æ±½è½¦ç”µå­"),
                            QuestionOption(id="consumer", label="æ¶ˆè´¹ç”µå­"),
                            QuestionOption(id="industrial", label="å·¥ä¸šæ§åˆ¶"),
                            QuestionOption(id="medical", label="åŒ»ç–—å™¨æ¢°"),
                            QuestionOption(id="other", label="å…¶ä»–"),
                        ]
                    )
                )
        
        # æŠ¥ä»·æŸ¥è¯¢ï¼šéœ€è¦äº§å“ä¿¡æ¯
        if intent == IntentType.QUOTE:
            if "product" not in entities and "product" not in collected_answers:
                return MissingInfoAnalysis(
                    can_proceed=False,
                    missing_fields=[{"field": "product", "importance": "required"}],
                    next_question=DynamicQuestion(
                        field_id="product",
                        question="æ‚¨æƒ³äº†è§£å“ªæ¬¾äº§å“çš„æŠ¥ä»·ï¼Ÿ",
                        question_type=QuestionType.INPUT,
                        placeholder="ä¾‹å¦‚ï¼šAOI8000"
                    )
                )
        
        # è®¡ç®—ç±»ï¼šéœ€è¦å‚æ•°
        if intent == IntentType.CALCULATION:
            required = ["capacity", "power"]
            missing = [p for p in required if p not in entities and p not in collected_answers]
            if missing:
                field = missing[0]
                return MissingInfoAnalysis(
                    can_proceed=False,
                    missing_fields=[{"field": field, "importance": "required"}],
                    next_question=DynamicQuestion(
                        field_id=field,
                        question=f"è¯·è¾“å…¥{field}å‚æ•°",
                        question_type=QuestionType.INPUT,
                        placeholder="è¯·è¾“å…¥æ•°å€¼"
                    )
                )
        
        # é»˜è®¤ï¼šå¯ä»¥ç»§ç»­
        return MissingInfoAnalysis(
            can_proceed=True,
            optimized_query=""
        )
    
    def format_collected_info_for_display(
        self,
        collected_answers: Dict[str, Any],
        questions_asked: List[DynamicQuestion]
    ) -> str:
        """æ ¼å¼åŒ–å·²æ”¶é›†ä¿¡æ¯ç”¨äºæ˜¾ç¤º"""
        if not collected_answers:
            return ""
        
        lines = ["ğŸ“‹ å·²æ”¶é›†ä¿¡æ¯ï¼š"]
        for q in questions_asked:
            if q.field_id in collected_answers:
                answer = collected_answers[q.field_id]
                # å¦‚æœæ˜¯é€‰é¡¹ç±»å‹ï¼Œè½¬æ¢ä¸ºæ ‡ç­¾
                if q.question_type in (QuestionType.SINGLE, QuestionType.MULTI):
                    option_map = {opt.id: opt.label for opt in q.options}
                    if isinstance(answer, list):
                        answer = ", ".join([option_map.get(a, a) for a in answer])
                    else:
                        answer = option_map.get(answer, answer)
                lines.append(f"  â€¢ {q.question.rstrip('ï¼Ÿ?')}: {answer}")
        
        return "\n".join(lines)


# ==================== ä¾¿æ·å‡½æ•° ====================

_service_instance: Optional[DynamicInteractionService] = None


def get_dynamic_interaction_service(
    llm_client: Optional[OpenAI] = None
) -> DynamicInteractionService:
    """è·å–åŠ¨æ€äº¤äº’æœåŠ¡å®ä¾‹"""
    global _service_instance
    
    if _service_instance is None:
        _service_instance = DynamicInteractionService(llm_client)
    elif llm_client and not _service_instance.llm_client:
        _service_instance.llm_client = llm_client
    
    return _service_instance


def analyze_missing_info(
    intent_result: IntentResult,
    collected_answers: Dict[str, Any] = None,
    original_query: str = "",
    history: List[Dict] = None,
    llm_client: Optional[OpenAI] = None
) -> MissingInfoAnalysis:
    """ä¾¿æ·å‡½æ•°ï¼šåˆ†æç¼ºå¤±ä¿¡æ¯å¹¶ç”Ÿæˆé—®é¢˜"""
    service = get_dynamic_interaction_service(llm_client)
    return service.analyze_and_generate_question(
        intent_result, collected_answers, original_query, history
    )

